{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SketchANetFinalYear.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YEWm7ycnIEwM","colab_type":"code","outputId":"09a9c30f-f9a4-472b-e0db-67de21fece9b","executionInfo":{"status":"error","timestamp":1557651523179,"user_tz":300,"elapsed":1494964,"user":{"displayName":"Ronnie Antwiler II","photoUrl":"","userId":"03414689190242325478"}},"colab":{"base_uri":"https://localhost:8080/","height":1397}},"source":["import copy\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist, cifar10\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n","from keras.optimizers import SGD, RMSprop\n","from keras.utils import np_utils\n","from keras.regularizers import l2\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n","from keras.callbacks import EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers.normalization import BatchNormalization\n","from keras.callbacks import ModelCheckpoint\n","from PIL import Image\n","\n","## Model check point\n","filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n","# filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","## File path to read data\n","mypath ='./training50classes/'\n","mypath2 ='./validation50classes/'\n","\n","## Model\n","batch_size = 32\n","nb_classes = 50\n","nb_epoch = 200\n","data_augmentation = True\n","\n","# sketch-A-Net\n","# apply a 3x3 convolution with 64 output filters on a 256x256 image:\n","model = Sequential()\n","model.add(Convolution2D(64, 15, 15, border_mode='same', input_shape=(225, 225, 1)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","model.add(Convolution2D(128, 5, 5, border_mode='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","model.add(Convolution2D(256, 3, 3, border_mode='same'))\n","model.add(Activation('relu'))\n","\n","model.add(Convolution2D(256,3,3, border_mode='same'))\n","model.add(Activation('relu'))\n","\n","model.add(Convolution2D(256,3,3, border_mode='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","model.add(Convolution2D(512,7,7, border_mode='same'))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Convolution2D(512,1,1, border_mode='same'))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Convolution2D(250,1,1, border_mode='same'))\n","model.add(Flatten())\n","model.add(Dense(nb_classes))\n","model.add(Activation('softmax'))\n","\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","print('Using real-time data augmentation.')\n","\n","# This will do preprocessing and realtime data augmentation:\n","datagen = ImageDataGenerator(\n","    featurewise_center=True,  # set input mean to 0 over the dataset\n","    samplewise_center=True,  # set each sample mean to 0\n","    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n","    samplewise_std_normalization=True,  # divide each input by its std\n","    zca_whitening=True,  # apply ZCA whitening\n","    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n","    horizontal_flip=True,  # randomly flip images\n","    vertical_flip=True)  # randomly flip images\n","\n","validation_datagen = ImageDataGenerator()\n","\n","train_generator = datagen.flow_from_directory(\n","    mypath,  # this is the target directory\n","    target_size=(225,225),  # all images will be resized to 150x150\n","    batch_size=batch_size,\n","    shuffle = True,\n","    color_mode='grayscale',\n","    class_mode='categorical')\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","       mypath2,  # this is the target directory\n","       target_size=(225,225),  # all images will be resized to 150x150\n","       batch_size=batch_size,\n","       shuffle = True,\n","       color_mode='grayscale',\n","       class_mode='categorical')\n","\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator(train_generator,\n","                    samples_per_epoch=20000,\n","                    nb_epoch=nb_epoch,\n","                    callbacks=callbacks_list, \n","                    validation_data = validation_generator,\n","                    nb_val_samples=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (15, 15), input_shape=(225, 225,..., padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (7, 7), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(250, (1, 1), padding=\"same\")`\n","/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:339: UserWarning: This ImageDataGenerator specifies `zca_whitening` which overrides setting of`featurewise_std_normalization`.\n","  warnings.warn('This ImageDataGenerator specifies '\n"],"name":"stderr"},{"output_type":"stream","text":["Using real-time data augmentation.\n","Found 3000 images belonging to 50 classes.\n","Found 1000 images belonging to 50 classes.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., validation_data=<keras_pre..., steps_per_epoch=625, epochs=200, validation_steps=2)`\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/200\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n","/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:718: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n"],"name":"stderr"},{"output_type":"stream","text":["625/625 [==============================] - 148s 236ms/step - loss: 15.7827 - acc: 0.0197 - val_loss: 15.8663 - val_acc: 0.0156\n","\n","Epoch 00001: val_acc improved from -inf to 0.01562, saving model to weights-improvement-01-0.02.hdf5\n","Epoch 2/200\n","625/625 [==============================] - 142s 228ms/step - loss: 15.7909 - acc: 0.0203 - val_loss: 15.6144 - val_acc: 0.0312\n","\n","Epoch 00002: val_acc improved from 0.01562 to 0.03125, saving model to weights-improvement-02-0.03.hdf5\n","Epoch 3/200\n","625/625 [==============================] - 146s 233ms/step - loss: 15.7992 - acc: 0.0198 - val_loss: 15.8663 - val_acc: 0.0156\n","\n","Epoch 00003: val_acc did not improve from 0.03125\n","Epoch 4/200\n","625/625 [==============================] - 143s 229ms/step - loss: 15.7949 - acc: 0.0201 - val_loss: 15.8663 - val_acc: 0.0156\n","\n","Epoch 00004: val_acc did not improve from 0.03125\n","Epoch 5/200\n","625/625 [==============================] - 146s 233ms/step - loss: 15.7917 - acc: 0.0203 - val_loss: 15.1107 - val_acc: 0.0625\n","\n","Epoch 00005: val_acc improved from 0.03125 to 0.06250, saving model to weights-improvement-05-0.06.hdf5\n","Epoch 6/200\n","625/625 [==============================] - 142s 228ms/step - loss: 15.8008 - acc: 0.0197 - val_loss: 15.8663 - val_acc: 0.0156\n","\n","Epoch 00006: val_acc did not improve from 0.06250\n","Epoch 7/200\n","625/625 [==============================] - 145s 232ms/step - loss: 15.8024 - acc: 0.0196 - val_loss: 16.1181 - val_acc: 0.0000e+00\n","\n","Epoch 00007: val_acc did not improve from 0.06250\n","Epoch 8/200\n","625/625 [==============================] - 144s 231ms/step - loss: 15.7855 - acc: 0.0206 - val_loss: 16.1181 - val_acc: 0.0000e+00\n","\n","Epoch 00008: val_acc did not improve from 0.06250\n","Epoch 9/200\n","625/625 [==============================] - 145s 233ms/step - loss: 15.8049 - acc: 0.0194 - val_loss: 15.8663 - val_acc: 0.0156\n","\n","Epoch 00009: val_acc did not improve from 0.06250\n","Epoch 10/200\n","625/625 [==============================] - 143s 228ms/step - loss: 15.7879 - acc: 0.0205 - val_loss: 16.1181 - val_acc: 0.0000e+00\n","\n","Epoch 00010: val_acc did not improve from 0.06250\n","Epoch 11/200\n","215/625 [=========>....................] - ETA: 1:31 - loss: 15.8057 - acc: 0.0194"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3b9c08769c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     nb_val_samples=2)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}